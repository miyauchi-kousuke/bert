{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sample1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miyauchi-kousuke/bert/blob/main/bert_sample1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zal1Q3YEodLX"
      },
      "source": [
        "# サンプル\n",
        "\n",
        "- 形態素解析用のjuman++をDLしてMAKEする\n",
        "- 日本語に最適化されたBertモデルを取得"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0PxAgrdnghP"
      },
      "source": [
        "# 形態素解析用のjuman++をDLしてMAKEする\n",
        "!wget https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc2/jumanpp-2.0.0-rc2.tar.xz\n",
        "!tar xfv jumanpp-2.0.0-rc2.tar.xz  \n",
        "%cd jumanpp-2.0.0-rc2\n",
        "!mkdir bld\n",
        "%cd bld\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\n",
        "!make install -j2\n",
        "!mkdir juman_bu\n",
        "!mkdir juman_bu/bin\n",
        "!mkdir juman_bu/libexec\n",
        "!cp -rf /usr/local/bin/jumanpp ./juman_bu/bin/\n",
        "!cp -rf /usr/local/libexec/jumanpp ./juman_bu/libexec/ \n",
        "\n",
        "# 日本語に最適化されたBertモデルを取得\n",
        "%cd\n",
        "%cd .././content\n",
        "!wget https://miyauchi-bert.s3-ap-northeast-1.amazonaws.com/Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers.zip\n",
        "!unzip Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDlSS9oovf9O"
      },
      "source": [
        "- 必要なライブラリをインストールする\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmcsWE3p85H"
      },
      "source": [
        "%cd\n",
        "%cd .././content\n",
        "!pip install pytorch-transformers\n",
        "!pip install transformers pyknp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB7NF2a0vq-q"
      },
      "source": [
        "- 形態素解析を実行する。\n",
        "- ここではテキストに入れた文言をjuman++を使って形態素解析する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68TR_wwsCHW"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertForMaskedLM, BertConfig, BertTokenizer\n",
        "from pyknp import Juman\n",
        "\n",
        "BASE_PATH = './Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers'\n",
        "BERT_CONFIG = 'config.json'\n",
        "BERT_MODEL = 'pytorch_model.bin'\n",
        "VOCAVULARY_LIST = 'vocab.txt'\n",
        "\n",
        "jumanpp = Juman()\n",
        "\n",
        "# 形態素解析\n",
        "text = 'どんなに勉強しても全然頭が良くならない'\n",
        "result = jumanpp.analysis(text)\n",
        "tokenized_text =[mrph.midasi for mrph in result.mrph_list()]\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfBbTKYv64a"
      },
      "source": [
        "- 上の処理で単語化されたものを、BERTに入れるために文頭と行間に指定されたsymbolを埋め込む\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGZWZLmtBN7"
      },
      "source": [
        "# Mask \n",
        "tokenized_text.insert(0, '[CLS]')\n",
        "tokenized_text.append('[SEP]')\n",
        "\n",
        "masked_index = 5 # Maskしたいtextのindex \n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB44q4fqwZ9a"
      },
      "source": [
        "- 予め学習モデルで使用した言語辞書を元に、単語をid化する。\n",
        "- そのidを元にTensor型(行列表現)を作る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syDkapWJtOMs"
      },
      "source": [
        "%cd\n",
        "%cd .././content\n",
        "# Bert model\n",
        "config = BertConfig.from_json_file(os.path.join(BASE_PATH, BERT_CONFIG))\n",
        "model = BertForMaskedLM.from_pretrained(os.path.join(BASE_PATH, BERT_MODEL), config=config)\n",
        "tokenizer = BertTokenizer(os.path.join(BASE_PATH, VOCAVULARY_LIST), do_lower_case=False, do_basic_tokenize=False)\n",
        "\n",
        "# token化\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "print(tokens_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JzIGTLuw9sA"
      },
      "source": [
        "- モデルを使ってマスク化された単語の予想をする\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Ka7_vHxFMM"
      },
      "source": [
        "# 予測\n",
        "model.eval()\n",
        "\n",
        "tokens_tensor = tokens_tensor.to('cpu')\n",
        "model.to('cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "_, predicted_indexes = torch.topk(predictions[0, masked_index], k=5)\n",
        "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
        "print(predicted_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eemrzMg7xKI_"
      },
      "source": [
        "以上"
      ]
    }
  ]
}