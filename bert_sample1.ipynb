{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sample1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miyauchi-kousuke/bert/blob/main/bert_sample1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zal1Q3YEodLX"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0PxAgrdnghP"
      },
      "source": [
        "!wget https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc2/jumanpp-2.0.0-rc2.tar.xz\n",
        "!tar xfv jumanpp-2.0.0-rc2.tar.xz  \n",
        "%cd jumanpp-2.0.0-rc2\n",
        "!mkdir bld\n",
        "%cd bld\n",
        "!cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\n",
        "!make install -j2\n",
        "!mkdir juman_bu\n",
        "!mkdir juman_bu/bin\n",
        "!mkdir juman_bu/libexec\n",
        "!cp -rf /usr/local/bin/jumanpp ./juman_bu/bin/\n",
        "!cp -rf /usr/local/libexec/jumanpp ./juman_bu/libexec/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqlaRuBfpBjj"
      },
      "source": [
        "%cd\n",
        "%cd .././content\n",
        "!wget https://miyauchi-bert.s3-ap-northeast-1.amazonaws.com/Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers.zip\n",
        "!unzip Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmcsWE3p85H"
      },
      "source": [
        "%cd\n",
        "%cd .././content\n",
        "!pip install pytorch-transformers\n",
        "!pip install transformers pyknp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68TR_wwsCHW"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertForMaskedLM, BertConfig, BertTokenizer\n",
        "from pyknp import Juman\n",
        "\n",
        "BASE_PATH = './Japanese_L-12_H-768_A-12_E-30_BPE_WWM_transformers'\n",
        "BERT_CONFIG = 'config.json'\n",
        "BERT_MODEL = 'pytorch_model.bin'\n",
        "VOCAVULARY_LIST = 'vocab.txt'\n",
        "\n",
        "jumanpp = Juman()\n",
        "\n",
        "# 形態素解析\n",
        "text = 'どんなに勉強しても全然頭が良くならない'\n",
        "result = jumanpp.analysis(text)\n",
        "tokenized_text =[mrph.midasi for mrph in result.mrph_list()]\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGZWZLmtBN7"
      },
      "source": [
        "# Mask \n",
        "tokenized_text.insert(0, '[CLS]')\n",
        "tokenized_text.append('[SEP]')\n",
        "\n",
        "masked_index = 5 # Maskしたいtextのindex \n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syDkapWJtOMs"
      },
      "source": [
        "%cd\n",
        "%cd .././content\n",
        "# Bert model\n",
        "config = BertConfig.from_json_file(os.path.join(BASE_PATH, BERT_CONFIG))\n",
        "model = BertForMaskedLM.from_pretrained(os.path.join(BASE_PATH, BERT_MODEL), config=config)\n",
        "tokenizer = BertTokenizer(os.path.join(BASE_PATH, VOCAVULARY_LIST), do_lower_case=False, do_basic_tokenize=False)\n",
        "\n",
        "# token化\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "print(tokens_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}